{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nimport shap\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBClassifier,XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nimport numpy as np\nimport pandas as pd\n\n# Feature importance is one way of determining what affects the target variable.\n# We can utilise various methods/libraries to help us explore the relative importance of \n# features and get a sense of how accurate the matrix correlation values are.\n# Different approaches are likely to predict different top features, so let's focus on the\n# less relevant features only.\n# Let's use the function from the Building an Asset Trading Strategy notebook, I found it \n# was much more useful than unsupervised learning dimensionality reduction, when it comes \n# to feature importance evaluation.\n  \nline_colors = [\"#7CEA9C\", '#50B2C0', \"rgb(114, 78, 145)\", \n               \"hsv(348, 66%, 90%)\", \"hsl(45, 93%, 58%)\"]\n            \n# Plot Correlation\ndef corrMat2(df,feature='target',figsize=(9,0.5),ret_id=False):\n\n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == feature].transpose().copy()\n\n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                    cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {feature}')\n\n    if(ret_id):\n        return corr\n\n''' Plot Relative Feature Importance '''\ndef feature_importance(tldf,feature='target',n_est=500,figsize=[800,400]):\n\n    # Select Numerical Features only & drop NaN\n    ldf0 = tldf.select_dtypes(include=['float64','int64','uint8'])\n    ldf = ldf0.dropna()\n\n    # Input dataframe containing feature & target variable\n    X = ldf.copy()\n    y = ldf[feature].copy()\n    del X[feature]\n\n#   CORRELATION\n    imp = corrMat2(ldf,feature,figsize=(15,0.5),ret_id=True)\n    del imp[feature]\n    s1 = imp.squeeze(axis=0);s1 = abs(s1)\n    s1.name = 'Correlation'\n\n#   SHAP\n    model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    shap_sum = np.abs(shap_values).mean(axis=0)\n    s2 = pd.Series(shap_sum,index=X.columns,name='Cat_SHAP').T\n\n#   RANDOMFOREST\n    model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)\n    fit = model.fit(X,y)\n    rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                        columns=['RandForest']).sort_values('RandForest',ascending=False)\n    s3 = rf_fi.T.squeeze(axis=0)\n\n#   XGB \n    model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)\n    model.fit(X,y)\n    data = model.feature_importances_\n    s4 = pd.Series(data,index=X.columns,name='XGB').T\n\n#   KBEST\n    model = SelectKBest(k=X.shape[1], score_func=f_regression)\n    fit = model.fit(X,y)\n    data = fit.scores_\n    s5 = pd.Series(data,index=X.columns,name='K_best')\n\n    # Combine Scores\n    df0 = pd.concat([s1,s2,s3,s4,s5],axis=1)\n    df0.rename(columns={'target':'lin corr'})\n\n    x = df0.values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)\n    df = df.rename_axis('Feature Importance via', axis=1)\n    df = df.rename_axis('Feature', axis=0)\n    df['total'] = df.sum(axis=1)\n    df = df.sort_values(by='total',ascending=True)\n    del df['total']\n    fig = px.bar(df,orientation='h',barmode='stack',color_discrete_sequence=line_colors)\n    fig.update_layout(template='plotly_white',height=figsize[1],width=figsize[0],margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\n    for data in fig.data:\n        data[\"width\"] = 0.6 #Change this value for bar widths\n    #fig.show()\n    fig.write_html(\"/kaggle/working/feature_importance.html\")","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Example Application '''\nfrom sklearn import datasets\n\ndef sklearn_to_df(sklearn_dataset):\n    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n    df['target'] = pd.Series(sklearn_dataset.target)\n    return df\n\n#df_boston = sklearn_to_df(datasets.load_boston())\n#display(df_boston.head())\n#df_cali = sklearn_to_df(datasets.fetch_california_housing())\n#display(df_cali.head())\ndf_diab = sklearn_to_df(datasets.load_diabetes())\n# print(df_diab.isna().sum()) # check missing data in columns\n\nfeature_importance(df_diab,feature='target')","execution_count":35,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}