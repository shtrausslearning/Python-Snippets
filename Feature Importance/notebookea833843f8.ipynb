{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os,sys,warnings\nif not sys.warnoptions:\n    warnings.simplefilter('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ipywidgets as widgets\nfrom plotly import tools\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.base import BaseEstimator, RegressorMixin,ClassifierMixin\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n#https://plotly.com/python/facet-plots/\n#https://plotly.com/python/facet-plots/#wrapping-column-facets\n #   https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\ndef impute_knn(df):\n    \n    ''' inputs: pandas df containing feature matrix '''\n    ''' outputs: dataframe with NaN imputed '''\n    # imputation with KNN unsupervised method\n\n    # separate dataframe into numerical/categorical\n    ldf = df.select_dtypes(include=[np.number])           # select numerical columns in df\n    ldf_putaside = df.select_dtypes(exclude=[np.number])  # select categorical columns in df\n    # define columns w/ and w/o missing data\n    cols_nan = ldf.columns[ldf.isna().any()].tolist()         # columns w/ nan \n    cols_no_nan = ldf.columns.difference(cols_nan).values     # columns w/o nan \n\n    for col in cols_nan:                \n        imp_test = ldf[ldf[col].isna()]   # indicies which have missing data will become our test set\n        imp_train = ldf.dropna()          # all indicies which which have no missing data \n        model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach\n        knr = model.fit(imp_train[cols_no_nan], imp_train[col])\n        ldf.loc[df[col].isna(), col] = knr.predict(imp_test[cols_no_nan])\n    \n    return pd.concat([ldf,ldf_putaside],axis=1)\n\ndef bar_plot(x, y,palette_len,title='Missing Values (%)', xlim = None, ylim = None, \n             xticklabels = None, yticklabels = None,xlabel = None, ylabel = None, \n             figsize = (10,4),axis_grid = 'y'):\n        \n    cmap = sns.color_palette(\"plasma\")\n    fig, ax = plt.subplots(figsize = figsize)\n    plt.title(title)\n\n    for i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_color('black')\n    \n    ax.spines['top'].set_visible(True);ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n\n    sns.barplot(x = x, y = y, edgecolor = 'black', ax = ax,\n                palette = cmap)\n    ax.set_xlim(xlim);ax.set_ylim(ylim)    \n    ax.set_xticklabels(xticklabels);ax.set_yticklabels(yticklabels)\n    plt.xlabel(xlabel);plt.ylabel(ylabel)\n    ax.grid(axis = axis_grid,ls='--',alpha = 0.9)\n    plt.show()\n\n# Plot Correlation Matrix\ndef corrMat(df,id=False,size=None):\n    if(size is not None):\n        figsize = (9,7)\n    else:\n        figsize = size\n    corr_mat = df.corr().round(2)\n    f, ax = plt.subplots(figsize=size)\n    mask = np.triu(np.ones_like(corr_mat, dtype=np.bool))\n    mask = mask[1:,:-1]\n    corr = corr_mat.iloc[1:,:-1].copy()\n    sns.heatmap(corr,mask=mask,vmin=-0.3,vmax=0.3,center=0, \n                cmap='Blues',square=False,lw=2,annot=True,cbar=False)\n    ax.set_ylim(len(corr_mat)-0.5,-0.5)\n    \n# Plot Correlation to Target Variable only\ndef corrMat2(df,target='demand',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    \n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap='Blues',square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr\n    \n''' Draw a Bivariate Seaborn Pairgrid /w KDE density w/ '''\ndef snsPairGrid(df,title):\n\n    ''' Plots a Seaborn Pairgrid w/ KDE & scatter plot of df features'''\n    sns.set(style='whitegrid')\n    g = sns.PairGrid(df,diag_sharey=False)\n    g.fig.set_size_inches(14,14)\n    g.map_diag(sns.kdeplot, lw=2)\n    g.map_lower(sns.scatterplot,s=15,edgecolor=\"k\",linewidth=1.0,alpha=0.4)\n    g.map_lower(sns.kdeplot,cmap='plasma',n_levels=3)\n    g.set(xlim=(-0.2,0.2),ylim=(-0.2,0.2))\n    g.fig.suptitle(title, y=1.02)\n    plt.tight_layout()\n    \ndef plot_na(ldf):\n    naval = (ldf.isnull().sum()/len(ldf)*100).sort_values(ascending = False)\n    bar_plot(x = naval,y = naval.index,\n             palette_len = naval.index,xlim = (0,1),xticklabels = range(0,10,1),\n             yticklabels = naval.index,figsize = (10,5), axis_grid = 'x')\n        \n# option to outline some regions in a plotly figure\ndef outline_plot(ldf=None,fig=None,var=None,level=0,mode='above',layer='below',verbose=False):\n\n    assert(ldf is not None)\n    assert(fig is not None)\n    assert(var is not None)\n\n    fillcolour = 'rgba(100,100,100,0.2)'\n    if mode == 'above':\n        m = ldf[var].gt(level)\n    if mode == 'below':\n        m = ldf[var].lt(level)\n\n    ldf['Date'] = ldf.index\n    df1 = ldf[m].groupby((~m).cumsum())['Date'].agg(['first','last'])\n\n    for index, row in df1.iterrows():\n        if(verbose):\n            print(row['first'], row['last'])\n        fig.add_shape(type=\"rect\",xref=\"x\", yref=\"paper\",x0=row['first'],y0=0,\n                        x1=row['last'],y1=1,line=dict(color=\"rgba(0,0,0,0)\",width=3,),\n                        fillcolor=fillcolour,layer=layer) \n    return(fig)\n\n# One plot type plotly from dataframe\ndef plot_line(ldf,lst,title='',sec_id=None,height=350,trace_only=False,\n              outline=False,add_bar=False,zoom=False,r_zoom=['2019','2020']):\n    \n    # sec_id - list of [False,False,True] values of when to activate \n    #           supblots; same length as lst\n    \n    # Always make new plot\n    if(trace_only is False):\n        if(sec_id is not None):\n            fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n        else:\n            fig = go.Figure()\n        \n    if(len(lst) is not 1):\n        ii=-1\n        for i in lst:\n            ii+=1\n            if(sec_id is not None):\n                if(add_bar):\n                    fig.add_bar(x=ldf.index,y=ldf[lst[ii]],name=lst[ii])\n                else:\n                    fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[ii]],mode='lines',\n                                         name=lst[ii],line=dict(width=2.0)),secondary_y=sec_id[ii])\n            else:\n                if(add_bar):\n                    fig.add_bar(x=ldf.index, y=ldf[lst[ii]],name=lst[ii])\n                else:\n                    fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[ii]],mode='lines',\n                                             name=lst[ii],line=dict(width=2.0)))\n    else:\n        if(add_bar):\n            fig.add_bar(x=ldf.index,y=ldf[lst[0]],name=lst[0])\n        else:\n            fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[0]],mode='lines',\n                                     name=lst[0],line=dict(width=2.0)))\n            \n    # Outline is an indicator of non final plot\n    fig.update_layout(height=height,template='plotly_white',title=title,\n                      margin=dict(l=0,r=0,t=30,b=0))\n    if(zoom):\n        fig.update_xaxes(type=\"date\", range=[r_zoom[0],r_zoom[1]])\n            \n    if(outline is False):\n        fig.show()\n    else:\n        return fig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:#5D2ECC'>Introduction</span>\n\n- According to the **Clean Energy Australia Report 2020**, Victoria had a renewable energy penetration of 23.9% in 2019\nwhich indicates that the majority comes from **fossil fuels** such as its coal-fired power stations [[1]](https://assets.cleanenergycouncil.org.au/documents/resources/reports/clean-energy-australia/clean-energy-australia-report-2020.pdf)\n- Victoriaâ€™s 50 per cent renewable energy target by 2030 became law in October 2019, which indicates further transition into renewable energy will continue to occur in the next decade.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:#5D2ECC'>Feature Information</span>\nA description of all the relevant features in the dataset\n\n<b>date</b> : The date of the recording <br>\n<b>school_day</b> : If students were at school on that day <br>\n<b>holiday</b> : If the day was a state or national holiday <br>\n    \n## <span style='color:#5D6D7E'>Electricity Demand & Breakdown </span>\n\n<b>demand</b> : Total daily electricity demand in MWh <br>\n    \n<b>RRP</b> : Recommended retail price in AUD/MWh <br>\n<b>demand_pos_RRP</b> : A total daily demand at positive RRP in MWh <br>\n<b>demand_neg_RRP</b> : Total daily demand at negative RRP in MWh <br>\n    \n<b>RRP_positive</b> : Averaged positive RRP, weighted by the corresponding intraday demand in AUD/MWh <br>\n<b>RRP_negative</b> : Average negative RRP, weighted by the corresponding intraday demand in AUD/MWh <br>\n<b>frac_at_neg_RRP</b> : A fraction of the day when the demand was traded at negative RRP <br>\n    \n## <span style='color:#5D6D7E'>Weather Related Features</span>\n<b>min_temperature</b> : Minimum temperature during the day in Celsius <br>\n<b>max_temperature</b> : Maximum temperature during the day in Celsius <br>\n<b>solar_exposure</b> : Total daily sunlight energy in MJ/m^2 <br>\n<b>rainfall</b> : Daily rainfall in mm <br>\n\nSeptember,October,November - Spring <br>\nDecember,January,February - Summer <br>\nMarch,April,May - Autumn <br>\nJune,July,August - Winter\n\n"},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:#5D2ECC'>Dataset Tasks</span>\n*The dataset comes with two tasks:*\n\n- **Electricity demand forecasting** is vital for our daily lives as it allows energy generation to match our needs without too much waste. How accurately is it possible to **forecast the demand for a day or a week ahead**?\n\n- **Negative electricity** prices are damaging for electricity companies. Is it possible to predict them or determine the main factors responsible for **prolonged daily periods** of negative prices?"},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:#5D2ECC'>The Dataset</span>\n\nLet's make some preliminary insights into the dataset:\n\n- The dataset comes with __14 features__, one of which is date time.\n- The dataset has a few missing features in columns **solar_exposure & rainfall** \n- Dataset time period: **1st JAN 2015** to **6th OCT 2020**\n- Several features have very similar names (eg.`demand`, `demand_pos_RRP`, `demand_neg_RRP`); could have too high correlation to one another, which would make the model process pointless.\n- Our features have a wide range of magnitudes, it will be difficult to compare them to one another without `scaling`\n- Two features `school_day` & `holiday` use `Y/N`, which we'll replace with `1/0`"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/electricity-demand-in-victoria-australia/complete_dataset.csv'\ndf = pd.read_csv(path)\ndf.index = df['date']; del df['date']\n\n# Let's use the 0/1 system instead of N/Y\ndf.replace({'school_day':{'N':0, 'Y':1}},inplace=True)\ndf.replace({'holiday':{'N':0, 'Y':1}},inplace=True)\ndf.index = pd.to_datetime(df.index)\n\n# It's easier to work in GWh\npd.set_option(\"precision\",5)\nlst = ['demand','RRP','demand_pos_RRP','demand_neg_RRP','RRP_positive','RRP_negative']\ndf[lst].apply(lambda x: x/1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='color:#5D6D7E'>Missing Data</span>\n- We only have missing data for <code>__rainfall__</code> & <code>__solar exposure__</code>, the rest of the data is available to us. \n- Missing data with kNN unsupervised learning methodology."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set(style='whitegrid',font_scale=1)\nplot_na(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's show the missing data\npd.set_option(\"precision\",3)\ndf[df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally Impute the missing data with knn\ndf_impute = impute_knn(df)\ndel df; df = df_impute\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:#5D2ECC'>Electricity demand forecasting</span>\n\nLet's try the __first task__ in this dataset.\n\n> **Electricity demand forecasting** is vital for our daily lives as it allows energy generation to match our needs without too much waste. How accurately is it possible to **forecast the demand for a day or a week ahead**?\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\ndef plotlyoff_corr(corr,size=None):\n    \n    xcols = corr.columns.tolist();ycols = xcols\n    if(size is None):\n        width = 700; height = 500\n    else:\n        width = size[0]; height = size[1]\n    \n    layout = dict(\n        width = width,height = height,\n        yaxis= dict(tickangle=-30,side = 'left'),\n        xaxis= dict(tickangle=-30,side = 'top'))\n    fig = ff.create_annotated_heatmap(\n        z=corr.values,x= xcols,y= ycols,\n        colorscale='viridis',showscale=False)\n    fig['layout'].update(layout)\n    fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\n    \n    return py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. <span style='color:#82409C'>Exploratory Data Analysis</span>\n### <span style='color:#5D6D7E'>EDA: Pearson Correlation</span>\n- <b>Demand</b> & <b>Demand_pos_RRP</b> are quite strongly correlated to one another (0.97) (let's plot them below)\n- Most features a correlation of 0-0.26 to the target variable `demand`\n- The higher the __demand__"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotlyoff_corr(df_impute.corr().round(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that both features are quite similar, so it makes little sense to keep it, let's remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_line(df['2019':'2021'],['demand','demand_pos_RRP'],height=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = df_impute.drop('demand_pos_RRP',axis=1)\ndf0.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:#5D6D7E'> RRP Relation to demand (Time Series) </span>\n- Let's take a look at two resampled data relations, __5 days__ & __15 days__.\n- We can note some correlation of RRP to demand, __mainy for spike demand peaks__, however when we look at the overall tren, there seems to be specific period during which, the <code>demand</code> can be roughly split into three quarters, __2015-2017__, during which it was lower, a period when it was higher, __2017-2020__, and again lower in 2020 onwards, however the demand doesn't quite change in the same way.\n- Demand tends to be reducing, especially notable when we compare cyclic high demand peeks in __June-July__."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0 = df0.copy()\nconditions = [(df_0.index < '2016-12-31'),\n             (df_0.index > '2017-01-01') & (df_0.index < '2019-12-01'),\n             (df_0.index > '2019-12-2')]\nvalues = ['0','1','2']\ndf_0['group'] = np.select(conditions, values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0_trend = df0.resample('5D').mean()\nplot_line(df0_trend,['demand','RRP'],sec_id=[False,True])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0_trend = df0.resample('6M').mean()\nplot_line(df0_trend,['demand','RRP'],sec_id=[False,True])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:#5D6D7E'> RRP Relation to demand (Bivariate)</span>\nBivariate Relation of __RRP_positive__ & __RRP_negative__ to __demand__ (all data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = ['RRP_positive','RRP_negative']\nfig = make_subplots(rows=1, cols=2,shared_yaxes=False,subplot_titles=None)\n\nfig.add_trace(go.Scatter(y=df_0['RRP_positive'],x=df_0['demand'],color=df_0['group']),row=1,col=1)\nfig.add_trace(go.Scatter(y=df_0['RRP_negative'],x=df_0['demand']),row=1, col=2)\n\nfig.update_traces(mode='markers', marker_line_width=1, marker_size=5)\nfig.update_layout(template='plotly_white',height=500,showlegend=True)\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\nfig.update_xaxes(title_text='demand')\nfig.update_yaxes(range=[0,400], row=1, col=1)\nfig.update_yaxes(range=[-100,0], row=1, col=2)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"## <span style='color:#F1C40F'> Keeping Track of Dataframes </span>\n\n- df : Original DataFrame\n- df_impute : kNN imputed Original Data\n- df_impute2 : Removed demand_pos_RRP\n- tr_data1, tr_data2 : train/test split"},{"metadata":{},"cell_type":"markdown","source":"## <span style='color:#F1C40F'> Preliminary Models </span>\n\n> **Electricity demand forecasting** is vital for our daily lives as it allows energy generation to match our needs without too much waste. How accurately is it possible to **forecast the demand for a day or a week ahead**\n\n### <span style='color:#5D6D7E'> Cross Validation on Training Data </span>\nFirstly evaluating the cross_val_score on the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split for TimeSeries\ndef TimeSeries_Split(ldf,feature=None,split_id=[None,None],cut_id=None,plot_id=False,show_eval=False):\n    \n    # Reduce the number of used data\n    if(cut_id is not None):\n        print('Dataset Maximum Index Number Is Reduced')\n        ldf = ldf.iloc[-cut_id:]\n        t1 = ldf.index.max();t0 = ldf.index.min()\n        print(f'Dataset Min.Index: {t0} | Max.Index: {t1}')\n        \n    if(split_id[0] is not None):\n        # General Percentage Split (Non Shuffle requied for Time Series)\n        train_df,eval_df = train_test_split(ldf,test_size=split_id[0],shuffle=False)\n    elif(split_id[1] is not None):\n        # specific time split \n        train_df = df.loc[:split_id[1]]; eval_df = df.loc[split_id[1]:] \n    else:\n        print('Choose One Splitting Method Only')\n        \n    if(plot_id):\n         \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=train_df.index, y=train_df[feature],\n                                 mode='lines',name='Training Data', line={'width': 2}))\n        fig.update_layout(template='plotly_white',title='Training Signal Visualisation',\n                          height=300,margin=dict(l=50,r=80,t=50,b=40))\n        if(show_eval):\n            fig.add_trace(go.Scatter(x=eval_df.index, y=eval_df[feature],\n                                    mode='lines',name='Test Data',line={'width': 2}))\n            fig.update_layout(title='Training/Test Signal Visualisation')\n        fig.show()\n        \n    return train_df,eval_df # return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data1, te_data1 = TimeSeries_Split(df_impute2,split_id=[0.1,None],plot_id=True,feature='demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Features '''\ntr_data1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Evaluation w/ Cross Validation\ndef eval1(ldf,feature='demand',eval_id='cv'):\n    \n    # Input: Feature & Target DataFrame\n    if(eval_id is 'trte'):\n        data1,data2 = TimeSeries_Split(ldf,split_id=[0.1,None])\n\n    # Split feature/target variable\n    y = ldf[feature].copy()\n    X = ldf.copy()\n    del X[feature]     # remove target variable\n    \n    # Pick Model \n    model = RandomForestRegressor(n_estimators=100,random_state=10)\n    if(eval_id is 'cv'):\n        cv_score = -cross_val_score(model,X,y,cv=5,scoring='neg_mean_squared_error')\n        print(f'cross_val_score: {cv_score.round(2)}')\n        print(f'cross_val_score mean: {cv_score.mean().round(2)}')\n    if(eval_id is 'trte'):\n        y1 = data1[feature].copy(); X1 = data1.copy(); del X1[feature]\n        y2 = data2[feature].copy(); X2 = data2.copy(); del X2[feature]\n        model.fit(X1,y1)\n        y_train = model.predict(X1); y_eval = model.predict(X2)\n        data1['train_model'] = y_train; data2['eval_model'] = y_eval\n        plot_line(data1,[feature,'train_model'])\n        plot_line(data2,[feature,'eval_model'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval1(tr_data1,eval_id='cv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval1(df_impute2,eval_id='trte')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy.fftpack import fft\n# import plotly.graph_objects as go\n# from plotly.subplots import make_subplots\n# import scipy\n\n# def plot_fft(ldf,name):\n#     signal_noise = ldf[name].values\n#     N = len(ldf[name])\n#     T = 1/N\n\n#     x = np.linspace(0.0, 1.0/(2.0*T), int(N/2))\n#     yr = fft(signal_noise) # \"raw\" FFT with both + and - frequencies\n#     y = 2/N * np.abs(yr[0:np.int(N/2)]) # positive freqs only\n#     return x[2:],y[2:]\n\n# # Create figure with secondary y-axis\n# fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# x1, y1 = plot_fft(RRP_feat_df,'solar_exposure')\n# x1, y2 = plot_fft(RRP_feat_df,'demand_neg_RRP')\n\n# # Add traces\n# fig.add_trace(go.Scatter(x=x1, y=y1, name=\"yaxis data\"),secondary_y=False)\n# fig.add_trace(go.Scatter(x=x1, y=y2, name=\"yaxis data\"),secondary_y=True)\n# fig.update_layout(height=300, width=1000,margin=dict(l=40, r=100, t=40, b=30),title='Weather Features : Frequency Domain') \n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}